{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_trees_CG_decision_trees.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jtHanMvu0UtJ"},"source":["# Решающие деревья\n","\n","![](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2017/01/B03905_05_01-compressor.png?resize=768%2C424)\n","\n","Решающие деревья - алгоритм МО, с помощью которого можно решать задачи классификации и регрессии. Основная идея алгортитма - это поиск условий принятия решений по тренировочным данным. У нас получается такое особое дерево поиска, в котором в узлах вопросы к данным на основе признаков, а в листовых узлах - метки классов. Во время обучения строится это дерево решений, причем так, чтобы оно было максимально эффективным - у него было хорошее качество предсказания и оно не было неоптимальным(слишком глубоким или несбалансированным).\n","\n","## Литература\n","\n","- [Habrahabr: ODS деревья решений](https://habrahabr.ru/company/ods/blog/322534/#derevo-resheniy)\n","- [ВМК МГУ семинары по решающим деревьям](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem04_trees.pdf)\n","- [Sklearn Decision Trees](http://scikit-learn.org/stable/modules/tree.html)"]},{"cell_type":"code","metadata":{"id":"K7RGNdj10UtP"},"source":["import sys\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from sklearn.tree import DecisionTreeClassifier\n","\n","%matplotlib inline\n","\n","# Зафиксируем случайность, чтобы каждый раз получалось одно и тоже\n","np.random.seed(seed=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dPE94rr00Uuq"},"source":["## Рассмотрим как строится дерево"]},{"cell_type":"code","metadata":{"id":"MIgNCxye0Uux"},"source":["from sklearn.tree import export_graphviz\n","import graphviz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSVIELgi0Uu0"},"source":["data = pd.DataFrame({'Возраст': [17,64,18,20,38,49,55,25,29,31,33], \n","             'Невозврат кредита': [1,0,1,0,1,0,0,1,1,0,1]})\n","\n","data.sort_values('Возраст')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPB4Nwzy0tcT"},"source":["clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(data['Возраст'].values.reshape(-1, 1), data['Невозврат кредита'].values)\n","\n","dot_data = export_graphviz(clf, feature_names=['Возраст'], filled=True, out_file=None)\n","graphviz.Source(dot_data) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITal1Ct30Uu9"},"source":["data2 = pd.DataFrame({'Возраст':  [17,64,18,20,38,49,55,25,29,31,33], \n","                      'Зарплата': [25,80,22,36,37,59,74,70,33,102,88], \n","             'Невозврат кредита': [1,0,1,0,1,0,0,1,1,0,1]})\n","\n","data2.sort_values('Зарплата')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-_pqaAv0UvB"},"source":["clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(data2[['Возраст', 'Зарплата']].values, data2['Невозврат кредита'].values)\n","\n","dot_data = export_graphviz(clf, feature_names=['Возраст', 'Зарплата'], filled=True, out_file=None)\n","graphviz.Source(dot_data) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u6yxJ9om1fVk"},"source":["## Растим дерево"]},{"cell_type":"markdown","metadata":{"id":"OW6HtVRl6FGA"},"source":["Прежде всего, для этого нам понадобится метрика неопределённости (impurity metric). \n","\n","Для классификации чаще всего предлагают следующие две:\n","<!-- \n","![picture](https://drive.google.com/uc?export=view&id=1DjsVDWXvKPs-cLbl8y9o-OozD_a7u1U-) -->\n","\n","- энтропия: $-\\sum_k{p_{ik}log_2(p_{ik})}$\n","\n","- индекс Джини: $\\sum_{j\\neq k}p_{ij}p_{ik} = 1 - \\sum_k p_{ik}^2$\n","\n","где $i$ - ID узлов дерева;\n","\n","$k, j$ - идентификаторы классов, представленных в датасете;\n","\n","$p_{ik} = \\frac{N_{ik}}{N_i}$, \n","\n","$p_{ik}$ - вероятность принадлежности класса $k$ узлу $i$,\n","\n","$N_{ik}$ - число объектов класса $k$ в узле $i$,\n","\n","$N_{i}$ - общее число объектов в узле $i$.\n"]},{"cell_type":"code","metadata":{"id":"ClTJuvlQ0c92"},"source":["def gini_index(y):\n","    # для каждого лейбла посчитайте вероятность его появления (нужно просуммировать с помощью numpy\n","    # значения из y, равные label, то есть y==label, затем поделить их на количество значений в y)\n","    # затем посчитайте итоговое значение по формуле выше, для возведения в степень используйте numpy\n","\n","    #################\n","    #### ВАШ КОД ####\n","    #################\n","\n","    pass\n","\n","def entropy(y):\n","    # вероятность вычисляется аналогично, для логарифма используйте np.log()\n","\n","    #################\n","    #### ВАШ КОД ####\n","    #################\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k20kqo7u20dZ"},"source":["### Опишем такую структуру данных, как узел в дереве.\n","Он будет содержать значение целевого признака, некоторые метаданные (номер колонки признака, величину примеси, ссылки на левый и правый потомок в дереве(на всякий случай)) и уметь делать предсказание.\n","\n","**Hint:** метод predict проверяет, является ли данный узел листом и в зависимости от этого либо возвращает значение, либо вызывает этот метод у дочерних узлов."]},{"cell_type":"code","metadata":{"id":"LXszAwNA7B_-"},"source":["class TreeNode:\n","    def __init__(self, impurity=sys.float_info.max, target_value=None):\n","        self.left_child = None  # левое ответвление\n","        self.right_child = None  # правое ответвление\n","        self.is_leaf = True  # флаг, является ли этот узел терминльным (то есть листом)\n","\n","        self.target_value = target_value  # значение целевого признака, которое предсказывает этот узел дерева\n","        \n","        self.condition_column = None  # id столбца, по которому будет делаться ветвление в этом узле дерева\n","        self.condition_value = None  # значение, величины, по которой было сделано ветвление\n","        self.impurity = impurity  # значение неопределённости для этого узла\n","\n","    def predict(self, x):\n","\n","        if self.is_leaf:\n","            return self.target_value\n","\n","        mask = x[:, self.condition_column] >= self.condition_value\n","\n","        res = np.zeros(x.shape[0])\n","        res[mask] = self.right_child.predict(x[mask])\n","        res[~mask] = self.left_child.predict(x[~mask])\n","\n","        #################\n","        #### ВАШ КОД ####\n","        #################\n","\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dB2iMrgx7_Hm"},"source":["#### Теперь можно приступить к реализации алгоритма построения дерева.\n","\n","На псевдокоде он выглядит вот так:\n","```python\n","def build(L):\n","    create node t\n","    if the stopping criterion is True:\n","        assign a predictive model to t\n","    else:\n","        Find the best binary split L = L_left + L_right\n","        t.left = build(L_left)\n","        t.right = build(L_right)\n","    return t\n","```\n","Как вы можете заметить, он рекурсивный."]},{"cell_type":"markdown","metadata":{"id":"6isQAlwlniqR"},"source":["Сначала напишем функцию, осуществляющую формирование одного узла дерева (ту самую, которую мы будем вызывать рекурсивно).\n","\n","Прекращать дробление выборки и генерацию новых узлов будем, если\n","- В текущем узле все элементы относятся к одному классу\n","- Information Gain наилучшего возможного разбиения отрицательный\n","\n","Information Gain ($IG$) определяется как\n","\n","$ IG = S_{orig} - \\frac{N_{left}}{N}S_{left} - \\frac{N_{right}}{N}S_{right}$, где \n","\n","$S_{orig}$ - неопределённость для всей выборки на текущем шаге\n","\n","$S_{left}, S_{right}$ - неопределённость на частях текущего разбиения\n","\n","$\\frac{N_{i}}{N}$ - доля подвыборки для $i$-й части разбиения относительно исходной выборки\n","\n","В этой формуле $\\frac{N_{left}}{N}S_{left} + \\frac{N_{right}}{N}S_{right}$ - неопределённость для нового разбиения, давайте посчитаем его отдельно."]},{"cell_type":"code","metadata":{"id":"G2yj0J-TtV_6"},"source":["def new_split_impurity(s_left, s_right, impurity_metric):\n","\n","    #################\n","    #### ВАШ КОД ####\n","    #################\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6-G3TJftV_6"},"source":["a = np.asarray([0,0,1,1,1])\n","print('Default impurity is', gini_index(a))\n","l, r = np.split(a, [2])\n","print(f'Impurity after splitting into {l} and {r} is', new_split_impurity(l, r, gini_index))\n","del a, l, r"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N93QihvrtV_7"},"source":["Реализуйте функцию выбора класса для текущего листа.\n","Будет достаточно выбрать наиболее часто встречающийся класс, или один из таких.\n","\n","Если хотите, можете придумать или найти какой-нибудь более хитрый метод."]},{"cell_type":"code","metadata":{"id":"j-TBhvnvtV_7"},"source":["def find_dominant_class(y):\n","\n","    #################\n","    #### ВАШ КОД ####\n","    #################\n","\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcM90d4ltV_7"},"source":["print(f'The most common class is \"{find_dominant_class([1,2,\"кочерыжка\", 1,\"кочерыжка\", \"кочерыжка\"])}\"')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmxlBKlhtV_8"},"source":["#### Теперь реализация алгоритма\n","Реализуйте метод построения дерева на основе описания, указанного выше.\n","\n","Для этого Вам понадобится дополнительная функция, которая выбирает наиболее подходящее разбиение.\n","\n","Разбиение в данной вариации делается по одному столбцу в данных. (Соответственно, нужно сначала подготавливать столбец к поиску разбиений)"]},{"cell_type":"code","metadata":{"id":"42Za1FkDtV_8"},"source":["def get_split_values(x):\n","    # здесь нужно сформировать и вернуть значения, по которым алгоритм будет выбирать значение для разбиения\n","    # подумайте, как сделать это лучше всего?\n","    # P.S.: х - это один столбец из характеристических данных\n","\n","    #################\n","    #### ВАШ КОД ####\n","    #################\n","\n","    pass\n","        \n","\n","def find_best_split(x, y, impurity_metric):\n","    min_impurity = np.inf\n","    best_split_col = 0\n","    best_split_row = 0\n","    \n","    for col in range(x.shape[1]):\n","        \n","        #################\n","        #### ВАШ КОД ####\n","        #################\n","        \n","    return min_impurity, best_split_col, best_split_value\n","\n","\n","def build_next_node(x, y, impurity_metric):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85MTk_bz0UtZ"},"source":["## Визуализация предсказаний\n","\n","Рассмотрим как справляются с задачей классификации 3 алгоритма - линейный, ближайшие соседи и решающее дерево на синтетическом примере - две луны и кольца."]},{"cell_type":"code","metadata":{"id":"TxaApio80Utb"},"source":["# Вспомогательная функция для генерации точек на всей поверхности scatter plot\n","def get_grid(X, y, step=0.01):\n","    x_min, x_max = X.min() - 1, X.max() + 1\n","    y_min, y_max = y.min() - 1, y.max() + 1\n","    return np.meshgrid(np.arange(x_min, x_max, step),\n","                         np.arange(y_min, y_max, step))\n","\n","# Вспомогательная функция которая покрасит всё пространство\n","def fill_color(clf, X, y, cmap=plt.cm.RdYlBu, proba=True, step=0.01):\n","    xx, yy = get_grid(X, y, step)\n","    if proba and hasattr(clf, 'predict_proba'):\n","        predicted = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1].reshape(xx.shape)\n","    else:\n","        predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n","    plt.contourf(xx, yy, predicted, cmap=cmap, alpha=.8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ITz-dJf0UvG"},"source":["fill_color(clf, data2['Возраст'], data2['Зарплата'], cmap=plt.cm.RdYlBu, proba=False, step=0.2)\n","plt.scatter(data2['Возраст'], data2['Зарплата'], c=data2['Невозврат кредита'], cmap=plt.cm.RdYlBu, edgecolors='k')\n","plt.xlabel('Возраст')\n","plt.ylabel('Зарплата')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7yALzaur0Uth"},"source":["from sklearn.datasets import make_moons, make_circles, make_classification"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBg3loIt0Utn"},"source":["## Две Луны"]},{"cell_type":"code","metadata":{"id":"TcC5XB0V0Utp"},"source":["X, y = make_moons(n_samples=150, noise=0.3, random_state=42)\n","\n","plt.figure(figsize=(10,10))\n","\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_Pw2Ob40Utv"},"source":["Для начала посмотрим, как с задачей справится линейная регрессия. Дополнительно вызвав функцию `fill_color` покрасим все пространство, чтобы видеть как меняется решение модели, на основе плавного изменения признка.  \n","Понятно, почему мы видим линии как разделители поверхностей. У нас же линейная модель!"]},{"cell_type":"code","metadata":{"id":"-AgKdvTo0Utx"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(solver='lbfgs')\n","clf.fit(X, y)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I3lzfkk80Ut4"},"source":["А K ближайших соседей работает немного по-другому. Видно, что это модель может уловить нелинейную взаимосвзять между признаками и целевым признаком. Пространство предсказания разукрасилось плавно."]},{"cell_type":"code","metadata":{"id":"j2pzWL7D0Ut5"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","clf = KNeighborsClassifier(7)\n","clf.fit(X, y)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu, proba=False)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"psAKCQiC0Ut_"},"source":["Решающие деревья разбивают пространство на прямоугольники, \"выцепляя\" объекты."]},{"cell_type":"code","metadata":{"id":"-0y8dogy0UuB"},"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","clf = DecisionTreeClassifier()\n","clf.fit(X, y)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhQGh2me03Dd"},"source":["clf = build_next_node(X, y, gini_index)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBoskghPUlMd"},"source":["clf = build_next_node(X, y, entropy)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"volBxb6T0UuH"},"source":["## Кольца"]},{"cell_type":"code","metadata":{"id":"oMMQVQ5R0UuK"},"source":["X, y = make_circles(n_samples=150, noise=0.2, factor=0.5, random_state=1)\n","\n","plt.figure(figsize=(10,10))\n","\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44bw953j0UuS"},"source":["Линейной модели совсем сложно справиться с такой задачей, так как объекты невозможно отделить одной прямой линией. (Но если применить трюк с переходом в другие координаты......)"]},{"cell_type":"code","metadata":{"id":"rocJO6uu0UuT"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","clf = LogisticRegression(solver='lbfgs')\n","clf.fit(X, y)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6guqdzo0UuZ"},"source":["clf = KNeighborsClassifier(7)\n","clf.fit(X, y)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu, proba=False)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzYTrIHk0Uud"},"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","clf = DecisionTreeClassifier()\n","clf.fit(X, y)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0y5oOu8fV28"},"source":["clf = build_next_node(X, y, gini_index)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CaqBGcsVAVf"},"source":["clf = build_next_node(X, y, entropy)\n","\n","plt.figure(figsize=(10,10))\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=150)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2K-crG6v0Uuk"},"source":["## Неустойчивость деревьев\n","\n","Неустойчивость к изменениям в данных.\n","\n","В отличие от весьма стабильных алгоритмов kNN и линейной классификации, дерево решений очень сильно меняется в зависимости от фазы луны и формы данных. Если мы добавим еще один элемент, то может всё сильно измениться, давайте посмотрим!\n","\n","**Спойлер**: но этот недостаток делает их сильнее, когда их много!!"]},{"cell_type":"code","metadata":{"id":"uK1T4Yg80Uul"},"source":["X, y = make_circles(n_samples=150, noise=0.2, factor=0.5, random_state=1)\n","\n","plt.figure(figsize=(16,16))\n","\n","\n","#################### Tree ##################\n","\n","clf = build_next_node(X[10:], y[10:], gini_index)\n","plt.subplot(221)\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=20)\n","\n","clf = build_next_node(X[:140], y[:140], gini_index)\n","plt.subplot(222)\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=20)\n","\n","#################### KNN ########################\n","\n","clf = KNeighborsClassifier(5)\n","clf.fit(X[10:], y[10:])\n","plt.subplot(223)\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu, proba=False)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=20)\n","\n","clf = KNeighborsClassifier(5)\n","clf.fit(X[:140], y[:140])\n","plt.subplot(224)\n","fill_color(clf, X, y, cmap=plt.cm.RdYlBu, proba=False)\n","plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black', s=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4paZIsd76lWe"},"source":["## Регрессия\n"]},{"cell_type":"markdown","metadata":{"id":"HphVk8Riypj2"},"source":["Теперь реализуем метрику под названием Остаточная Сумма Квадратов (Residual sum of squares).\n","\n","$RSS = \\sum_{j=1}^N(y_j - \\mu_{[j]})^2$\n","\n","где $\\mu_{[j]}$ - это среднее всех значений в листе из разбиения, к которому принадлежит $j$-й элемент."]},{"cell_type":"code","metadata":{"id":"3uNUX6n6y9Xd"},"source":["def residual_sum_of_squares(y):\n","\n","    #################\n","    #### ВАШ КОД ####\n","    #################\n","    \n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8x6rm0Layz_3"},"source":["Теперь настало время переделать нашу функцию под регрессию. Первая и самая важная вещь, которую стоит помнить это то, что в отличие от классификации, здесь все значения могут быть уникальными и их может быть бесконечно много (если, конечно, игнорировать ограничения, диктуемые нам \"железом\").\n","\n","Давайте вспомним наши критерии останова, взятые для классификации:\n","- В текущем узле все элементы относятся к одному классу\n","- Information Gain наилучшего возможного разбиения отрицательный\n","\n","Сразу же можно понять, что первый критерий теперь не подходит нам, т.к. он всегда будет приводить нас к переобученному дереву (тому, у которого на каждый элемент выборки приходится свой отдельный лист)\n","В такой ситуации возможны, как минимум, три выхода:\n","1. Оставить его как есть и получать в конце переобученное дерево (впрочем, в sklearn так и делают)\n","2. Ввести дополнительные ограничения, например на глубину дерева, минимальное количество объектов, относящихся к каждому листу дерева (есть в sklearn как опции)\n","3. Останавливать обучение, когда достигнута какая-то величина метрики неопределённости - по сути говоря, это почти то же самое, что задавать максимальную допустимую величину разности между двумя произвольными объектами в этом разбиении. Такой способ не требует никаких дополнительных условий, но это не очень надёжный способ, т.к. величина RSS напрямую зависит от масштаба характеристических признаков.\n","\n","Ещё можно \"обрезать\" переобученное дерево после построения, как советовали Брейман и Ко в описании алгоритма CART. Но это уже совсем другая история...\n","\n","Так, ладно, сначала попробуем натренировать дерево регрессии не меняя алгоритм. Оно должно получиться глубоким, но точным."]},{"cell_type":"markdown","metadata":{"id":"PB8rI-Jwy9m0"},"source":["Для этого загрузим датасет с ценами на жильё в Бостоне. Будем тестировать модель, предсказывая медианное значение цены на несъёмное жильё."]},{"cell_type":"code","metadata":{"id":"vIu4TtAP7SsK"},"source":["from sklearn.datasets import load_boston\n","\n","data = load_boston()\n","print(data.DESCR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjynGSFn80_o"},"source":["df = pd.DataFrame(data.data, columns=data.feature_names)\n","df['MEDV'] = data.target\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpocEihI83Ro"},"source":["from sklearn.model_selection import train_test_split\n","\n","X, X_test, y, y_test = train_test_split(df.drop(['MEDV'], axis=1), df['MEDV'], test_size=100, \n","                                        random_state=241)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8niQOuB7lMy"},"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","clf = DecisionTreeRegressor()\n","clf.fit(X, y)\n","\n","plt.scatter(y_test, y_test, color='green', s=1)  # идеальная прямая, чтобы было\n","plt.scatter(y, clf.predict(X), color='blue', s=5)\n","plt.scatter(y_test, clf.predict(X_test), color='red', s=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eoI7U-ehtWAK"},"source":["Выведем, сколько у дерева получилось листьев."]},{"cell_type":"code","metadata":{"id":"eFml1-XesrIt"},"source":["clf.get_n_leaves()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DE4nOLlxA5Fr"},"source":["Сейчас мы попробоуем запустить нашу старую функцию, которая предназначается для классификации. Там есть один важный нюанс, который скорее всего всё испортит, но всё равно давайте попробуем."]},{"cell_type":"code","metadata":{"id":"T7HAfE9A8IVf"},"source":["try:\n","    clf = build_next_node(X.to_numpy(), y.to_numpy(), residual_sum_of_squares)\n","\n","    plt.scatter(y_test, y_test, color='green', s=1)  # идеальная прямая, чтобы было\n","    plt.scatter(y, clf.predict(X.to_numpy()), color='blue', s=5)\n","    plt.scatter(y_test, clf.predict(X_test.to_numpy()), color='red', s=30)\n","except:\n","    print('A Fail has happened. We are doomed to implement a version for regression.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JOLqjML8tWAL"},"source":["Для нашего дерева нужен метод, чтобы посчитать листья, он уже готов."]},{"cell_type":"code","metadata":{"id":"y0DxBHQRs2TV"},"source":["def count_leaves(node):\n","    if node.is_leaf:\n","        return 1\n","    return count_leaves(node.left_child) + count_leaves(node.right_child)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxJvl8lrtUCs"},"source":["count_leaves(clf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jxc3Qe0ZtWAM"},"source":["А теперь давайте сделаем \"правильную\" версию с регрессией. Можно использовать старый метод, изменить нужно всего-лишь пару строк."]},{"cell_type":"code","metadata":{"id":"WyffIbXmtWAM"},"source":["def build_next_node_regression(x, y, impurity_metric):\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWO3u6ZaCaaO","scrolled":false},"source":["clf = build_next_node_regression(X.to_numpy(), y.to_numpy(), residual_sum_of_squares)\n","\n","plt.scatter(y_test, y_test, color='green', s=10)  # идеальная прямая, чтобы было\n","plt.scatter(y, clf.predict(X.to_numpy()), color='blue', s=5)\n","plt.scatter(y_test, clf.predict(X_test.to_numpy()), color='red', s=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FSm_10j6tgOV"},"source":["count_leaves(clf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h62s1oNatWAN"},"source":["### А что дальше?\n","\n","Над деревьями можно делать много разных модификаций:\n","- Урезать глубину дерева\n","- Запрещать создавать листья с менее чем $k$ элементами выборки на них\n","- модифицировать логику выбора класса, если на листе есть элементы разных классов\n","    - Вычислить вероятности выпадения каждого класса, тем самым задав распределение, и затем, при попадании на этот лист генерировать случайную величину из этого распределения, и считать это предсказанием\n","    - Так же, вычислить вероятности, а величину выбрать один раз сгенерировав случайную величину\n","- Объединить много таких деревьев в ансамбли (пожалуй, наиболее перспективная идея)\n"]}]}